

<!DOCTYPE html>
<!--[if IE 8]><html class="no-js lt-ie9" lang="en" > <![endif]-->
<!--[if gt IE 8]><!--> <html class="no-js" lang="en" > <!--<![endif]-->
<head>
  <meta charset="utf-8">
  
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  
  <title>layers &mdash; SimFlow 0.0.1 documentation</title>
  

  
  
  
  

  

  
  
    

  

  <link rel="stylesheet" href="../_static/css/theme.css" type="text/css" />
  <link rel="stylesheet" href="../_static/pygments.css" type="text/css" />
    <link rel="index" title="Index" href="../genindex.html" />
    <link rel="search" title="Search" href="../search.html" /> 

  
  <script src="../_static/js/modernizr.min.js"></script>

</head>

<body class="wy-body-for-nav">

   
  <div class="wy-grid-for-nav">

    
    <nav data-toggle="wy-nav-shift" class="wy-nav-side">
      <div class="wy-side-scroll">
        <div class="wy-side-nav-search">
          

          
            <a href="../index.html" class="icon icon-home"> SimFlow
          

          
          </a>

          
            
            
          

          
<div role="search">
  <form id="rtd-search-form" class="wy-form" action="../search.html" method="get">
    <input type="text" name="q" placeholder="Search docs" />
    <input type="hidden" name="check_keywords" value="yes" />
    <input type="hidden" name="area" value="default" />
  </form>
</div>

          
        </div>

        <div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="main navigation">
          
            
            
              
            
            
              <p class="caption"><span class="caption-text">Contents:</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../Summary.html">Summary</a></li>
<li class="toctree-l1"><a class="reference internal" href="../model.html">Model</a></li>
<li class="toctree-l1"><a class="reference internal" href="../layers.html">Layers</a></li>
<li class="toctree-l1"><a class="reference internal" href="../losses.html">Losses</a></li>
<li class="toctree-l1"><a class="reference internal" href="../iterators.html">Iterators</a></li>
<li class="toctree-l1"><a class="reference internal" href="../optimizers.html">Optimizers</a></li>
<li class="toctree-l1"><a class="reference internal" href="../data_loaders.html">Data Loaders</a></li>
</ul>

            
          
        </div>
      </div>
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap">

      
      <nav class="wy-nav-top" aria-label="top navigation">
        
          <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
          <a href="../index.html">SimFlow</a>
        
      </nav>


      <div class="wy-nav-content">
        
        <div class="rst-content">
        
          















<div role="navigation" aria-label="breadcrumbs navigation">

  <ul class="wy-breadcrumbs">
    
      <li><a href="../index.html">Docs</a> &raquo;</li>
        
          <li><a href="index.html">Module code</a> &raquo;</li>
        
      <li>layers</li>
    
    
      <li class="wy-breadcrumbs-aside">
        
      </li>
    
  </ul>

  
  <hr/>
</div>
          <div role="main" class="document" itemscope="itemscope" itemtype="http://schema.org/Article">
           <div itemprop="articleBody">
            
  <h1>Source code for layers</h1><div class="highlight"><pre>
<span></span><span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
<span class="kn">from</span> <span class="nn">im2col</span> <span class="k">import</span> <span class="o">*</span>
<div class="viewcode-block" id="Layer"><a class="viewcode-back" href="../layers.html#layers.Layer">[docs]</a><span class="k">class</span> <span class="nc">Layer</span><span class="p">(</span><span class="nb">object</span><span class="p">):</span>
    <span class="sd">&#39;&#39;&#39;</span>
<span class="sd">    Abstract class representing a neural network layer</span>
<span class="sd">    &#39;&#39;&#39;</span>
<div class="viewcode-block" id="Layer.forward"><a class="viewcode-back" href="../layers.html#layers.Layer.forward">[docs]</a>    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">X</span><span class="p">,</span> <span class="n">train</span><span class="o">=</span><span class="kc">True</span><span class="p">):</span>
        <span class="sd">&#39;&#39;&#39;</span>
<span class="sd">        Calculates a forward pass through the layer.</span>

<span class="sd">        Args:</span>
<span class="sd">            :X (numpy.ndarray):   Input to the layer with dimensions (batch_size, input_size)</span>
<span class="sd">            :train (bool):   If true caches values required for backward function</span>

<span class="sd">        Returns:</span>
<span class="sd">            :Out (numpy.ndarray):   Output of the layer with dimensions (batch_size, output_size)</span>
<span class="sd">        &#39;&#39;&#39;</span>
        <span class="k">raise</span> <span class="ne">NotImplementedError</span><span class="p">(</span><span class="s1">&#39;This is an abstract class&#39;</span><span class="p">)</span></div>

<div class="viewcode-block" id="Layer.backward"><a class="viewcode-back" href="../layers.html#layers.Layer.backward">[docs]</a>    <span class="k">def</span> <span class="nf">backward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">dY</span><span class="p">):</span>
        <span class="sd">&#39;&#39;&#39;</span>
<span class="sd">        Calculates a backward pass through the layer.</span>

<span class="sd">        Args:</span>
<span class="sd">            :dY (numpy.ndarray):   The gradient of the output with dimensions (batch_size, output_size)</span>

<span class="sd">        Returns:</span>
<span class="sd">            :dX (numpy.ndarray):   Gradient of the input (batch_size, output_size)</span>
<span class="sd">            :var_grad_list (list):   List of tuples in the form (variable_pointer, variable_grad)</span>
<span class="sd">        &#39;&#39;&#39;</span>
        <span class="k">raise</span> <span class="ne">NotImplementedError</span><span class="p">(</span><span class="s1">&#39;This is an abstract class&#39;</span><span class="p">)</span></div>

    <span class="k">def</span> <span class="nf">_initializer_</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span><span class="n">W</span><span class="p">,</span><span class="n">init_method</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Initializes the parameter passes as argument using Xavier of He initialization</span>

<span class="sd">        Args:</span>
<span class="sd">            W (numpy.ndarray): Parameter to be initialized</span>
<span class="sd">            init_method (str): Method to initialize the parameter</span>

<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">if</span> <span class="n">init_method</span> <span class="o">==</span> <span class="s1">&#39;Xavier&#39;</span><span class="p">:</span>
            <span class="k">if</span> <span class="nb">len</span><span class="p">(</span><span class="n">W</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span><span class="o">==</span><span class="mi">2</span><span class="p">:</span> <span class="c1">#linear layer</span>
              <span class="n">input_dim</span><span class="p">,</span><span class="n">output_dim</span> <span class="o">=</span> <span class="n">W</span><span class="o">.</span><span class="n">shape</span>
              <span class="k">return</span> <span class="n">np</span><span class="o">.</span><span class="n">sqrt</span><span class="p">(</span><span class="mf">2.0</span><span class="o">/</span><span class="p">(</span><span class="n">input_dim</span><span class="o">+</span><span class="n">output_dim</span><span class="p">))</span>
            <span class="k">elif</span> <span class="nb">len</span><span class="p">(</span><span class="n">W</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span><span class="o">==</span><span class="mi">4</span><span class="p">:</span><span class="c1">#convolutional layer</span>
              <span class="n">n_filter</span><span class="p">,</span><span class="n">d_filter</span><span class="p">,</span><span class="n">h_filter</span><span class="p">,</span><span class="n">w_filter</span> <span class="o">=</span> <span class="n">W</span><span class="o">.</span><span class="n">shape</span>
              <span class="k">return</span> <span class="n">np</span><span class="o">.</span><span class="n">sqrt</span><span class="p">(</span><span class="mf">2.0</span><span class="o">/</span><span class="p">(</span><span class="n">h_filter</span><span class="o">*</span><span class="n">w_filter</span><span class="o">*</span><span class="n">d_filter</span><span class="p">))</span>
            <span class="k">else</span><span class="p">:</span>
              <span class="k">raise</span> <span class="ne">NotImplementedError</span><span class="p">(</span><span class="s1">&#39;This W size is not defined&#39;</span><span class="p">)</span>
        <span class="k">elif</span> <span class="n">init_method</span> <span class="o">==</span> <span class="s1">&#39;He&#39;</span><span class="p">:</span>
            <span class="k">if</span> <span class="nb">len</span><span class="p">(</span><span class="n">W</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span><span class="o">==</span><span class="mi">2</span><span class="p">:</span> <span class="c1">#linear layer</span>
              <span class="n">input_dim</span><span class="p">,</span><span class="n">output_dim</span> <span class="o">=</span> <span class="n">W</span><span class="o">.</span><span class="n">shape</span>
              <span class="k">return</span> <span class="n">np</span><span class="o">.</span><span class="n">sqrt</span><span class="p">(</span><span class="mf">2.0</span><span class="o">/</span><span class="p">(</span><span class="n">input_dim</span><span class="p">))</span>
            <span class="k">elif</span> <span class="nb">len</span><span class="p">(</span><span class="n">W</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span><span class="o">==</span><span class="mi">4</span><span class="p">:</span><span class="c1">#convolutional layer</span>
              <span class="n">n_filter</span><span class="p">,</span><span class="n">d_filter</span><span class="p">,</span><span class="n">h_filter</span><span class="p">,</span><span class="n">w_filter</span> <span class="o">=</span> <span class="n">W</span><span class="o">.</span><span class="n">shape</span>
              <span class="k">return</span> <span class="n">np</span><span class="o">.</span><span class="n">sqrt</span><span class="p">(</span><span class="mf">2.0</span><span class="o">/</span><span class="p">(</span><span class="n">h_filter</span><span class="o">*</span><span class="n">w_filter</span><span class="o">*</span><span class="n">d_filter</span><span class="p">))</span>
            <span class="k">else</span><span class="p">:</span>
              <span class="k">raise</span> <span class="ne">NotImplementedError</span><span class="p">(</span><span class="s1">&#39;This W size is not defined&#39;</span><span class="p">)</span>
        <span class="k">else</span><span class="p">:</span>
          <span class="k">raise</span> <span class="ne">NotImplementedError</span><span class="p">(</span><span class="s1">&#39;This method not currently supported&#39;</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">__repr__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="k">if</span> <span class="nb">hasattr</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span><span class="n">l_name</span><span class="p">):</span>
            <span class="k">return</span> <span class="n">f</span><span class="s1">&#39;</span><span class="si">{self.l_name}</span><span class="s1"> layer&#39;</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="k">return</span> <span class="n">f</span><span class="s1">&#39;Layer&#39;</span>

<div class="viewcode-block" id="Layer.get_params"><a class="viewcode-back" href="../layers.html#layers.Layer.get_params">[docs]</a>    <span class="k">def</span> <span class="nf">get_params</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="sd">&#39;&#39;&#39;</span>
<span class="sd">        Returns the list of numpy array of weights</span>
<span class="sd">        &#39;&#39;&#39;</span>
        <span class="k">if</span> <span class="nb">hasattr</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span><span class="s1">&#39;params&#39;</span><span class="p">):</span>
            <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">params</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s1">&#39;Params not defined&#39;</span><span class="p">)</span></div>

<div class="viewcode-block" id="Layer.set_params"><a class="viewcode-back" href="../layers.html#layers.Layer.set_params">[docs]</a>    <span class="k">def</span> <span class="nf">set_params</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span><span class="n">params</span><span class="p">):</span>
        <span class="sd">&#39;&#39;&#39;</span>
<span class="sd">        Sets the params of a layer with a given list of numpy arrays</span>

<span class="sd">        Ags:</span>
<span class="sd">            :params (list of numpy.ndarray): new weights</span>

<span class="sd">        &#39;&#39;&#39;</span>
        <span class="n">old_params</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">get_params</span><span class="p">()</span>
        <span class="k">assert</span> <span class="nb">len</span><span class="p">(</span><span class="n">old_params</span><span class="p">)</span> <span class="o">==</span> <span class="nb">len</span><span class="p">(</span><span class="n">params</span><span class="p">),</span><span class="s2">&quot;length mismatch&quot;</span>
        <span class="k">assert</span> <span class="nb">all</span><span class="p">(</span><span class="n">params</span><span class="p">[</span><span class="n">i</span><span class="p">]</span><span class="o">.</span><span class="n">shape</span> <span class="o">==</span> <span class="n">old_params</span><span class="p">[</span><span class="n">i</span><span class="p">]</span><span class="o">.</span><span class="n">shape</span> <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">old_params</span><span class="p">))),</span><span class="s2">&quot;shape missmatch&quot;</span>
        <span class="k">for</span> <span class="p">(</span><span class="n">op</span><span class="p">,</span><span class="n">p</span><span class="p">)</span> <span class="ow">in</span> <span class="nb">zip</span><span class="p">(</span><span class="n">old_params</span><span class="p">,</span><span class="n">params</span><span class="p">):</span>
            <span class="n">op</span> <span class="o">=</span> <span class="n">p</span></div></div>



<div class="viewcode-block" id="Dense"><a class="viewcode-back" href="../layers.html#layers.Dense">[docs]</a><span class="k">class</span> <span class="nc">Dense</span><span class="p">(</span><span class="n">Layer</span><span class="p">):</span>
    <span class="sd">&#39;&#39;&#39;</span>
<span class="sd">    Dense / Linear Layer</span>

<span class="sd">    Represent a linear transformation Y = X*W + b</span>
<span class="sd">        :X: is an numpy.ndarray with shape (batch_size, input_dim)</span>
<span class="sd">        :W: is a trainable matrix with dimensions (input_dim, output_dim)</span>
<span class="sd">        :b: is a bias with dimensions (1, output_dim)</span>
<span class="sd">        :Y: is an numpy.ndarray with shape (batch_size, output_dim)</span>

<span class="sd">    Initialization:</span>
<span class="sd">        :W: initialized with either Xavier or He initialization</span>
<span class="sd">        :b: initialized to zero</span>

<span class="sd">        Args:</span>
<span class="sd">            :input_dim (int): size of input passed</span>
<span class="sd">            :output_dim (int): size of output requred</span>
<span class="sd">            :init_method (str): initialization method to be used for Weights</span>
<span class="sd">            :trainable (bool):</span>
<span class="sd">                :False: parameters of the layer are frozed</span>
<span class="sd">                :True: parameters are updated during optimizer step</span>
<span class="sd">    &#39;&#39;&#39;</span>
    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">input_dim</span><span class="p">,</span> <span class="n">output_dim</span><span class="p">,</span><span class="o">*</span><span class="p">,</span><span class="n">init_method</span><span class="o">=</span><span class="s1">&#39;Xavier&#39;</span><span class="p">,</span><span class="n">trainable</span> <span class="o">=</span> <span class="kc">True</span><span class="p">):</span>
        <span class="sd">&#39;&#39;&#39;</span>
<span class="sd">        Initializes the Desnse layer parameter</span>
<span class="sd">            :W: is initialized with either Xavier or He initialization</span>
<span class="sd">            :b: is initialized to zero</span>

<span class="sd">        Args:</span>
<span class="sd">            input_dim (int)   : size of input passed</span>
<span class="sd">            output_dim (int)  : size of output requred</span>
<span class="sd">            init_method (str) : initialization method to be used for Weights</span>
<span class="sd">            trainable (bool)  : if set to False parameters of the layer are frozed</span>
<span class="sd">                                if set to True parameters are updated during optimizer step</span>
<span class="sd">        &#39;&#39;&#39;</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">init_method</span> <span class="o">=</span> <span class="n">init_method</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">W</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="n">input_dim</span><span class="p">,</span> <span class="n">output_dim</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">W</span> <span class="o">*=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_initializer_</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">W</span><span class="p">,</span><span class="n">init_method</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">b</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">((</span><span class="mi">1</span><span class="p">,</span> <span class="n">output_dim</span><span class="p">))</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">cache_in</span> <span class="o">=</span> <span class="kc">None</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">trainable</span> <span class="o">=</span> <span class="n">trainable</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">l_name</span> <span class="o">=</span> <span class="s1">&#39;Dense&#39;</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">params</span> <span class="o">=</span> <span class="p">[</span><span class="bp">self</span><span class="o">.</span><span class="n">W</span><span class="p">,</span><span class="bp">self</span><span class="o">.</span><span class="n">b</span><span class="p">]</span>

<div class="viewcode-block" id="Dense.forward"><a class="viewcode-back" href="../layers.html#layers.Dense.forward">[docs]</a>    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">X</span><span class="p">,</span> <span class="n">train</span><span class="o">=</span><span class="kc">True</span><span class="p">):</span>
        <span class="sd">&#39;&#39;&#39;</span>
<span class="sd">        Performs a forward pass through the Dense Layer</span>

<span class="sd">        Args:</span>
<span class="sd">            :X (numpy.ndarray): Input array should be shape (batch_size x input_dim)</span>
<span class="sd">            :train (bool): Set to True to enable gardient caching for backward step</span>

<span class="sd">        Returns:</span>
<span class="sd">            :Out (numpy.ndarray): Output after applying transformation Y = X*W + b</span>
<span class="sd">                                  shape of output is (batch_size x output_dim)</span>
<span class="sd">        &#39;&#39;&#39;</span>
        <span class="k">assert</span> <span class="nb">len</span><span class="p">(</span><span class="n">X</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span><span class="o">==</span><span class="mi">2</span><span class="p">,</span><span class="s2">&quot;input dimenstions not supported&quot;</span>
        <span class="k">assert</span> <span class="n">X</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span><span class="o">==</span><span class="bp">self</span><span class="o">.</span><span class="n">W</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span><span class="n">f</span><span class="s2">&quot;input dimension doesn&#39;t match, each X has dimension </span><span class="si">{X.shape[1]}</span><span class="s2"> but Weights defined are of shape </span><span class="si">{self.W.shape[0]}</span><span class="s2">&quot;</span>
        <span class="n">out</span> <span class="o">=</span> <span class="n">X</span><span class="nd">@self</span><span class="o">.</span><span class="n">W</span> <span class="o">+</span> <span class="bp">self</span><span class="o">.</span><span class="n">b</span>
        <span class="k">if</span> <span class="n">train</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">cache_in</span> <span class="o">=</span> <span class="n">X</span>
        <span class="k">return</span> <span class="n">out</span></div>

<div class="viewcode-block" id="Dense.backward"><a class="viewcode-back" href="../layers.html#layers.Dense.backward">[docs]</a>    <span class="k">def</span> <span class="nf">backward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">dY</span><span class="p">):</span>
        <span class="sd">&#39;&#39;&#39;</span>
<span class="sd">        Performs a backward pass through the Dense Layer</span>

<span class="sd">        Args:</span>
<span class="sd">            :dY (numpy.ndarray): Output gradient backpropagated from layers in the front</span>
<span class="sd">                                  shape of dY is (batch_size x output_dim)</span>

<span class="sd">        Returns:</span>
<span class="sd">            :dX (numpy.ndarray): Input gradient after backpropagating dY through Dense layer</span>
<span class="sd">            :var_grad_list (list):</span>
<span class="sd">                :trainable = True: [(W,dW),(b,db)]</span>
<span class="sd">                :trainable = False: [ ]</span>
<span class="sd">        &#39;&#39;&#39;</span>
        <span class="n">dX</span> <span class="o">=</span> <span class="n">dY</span><span class="nd">@self</span><span class="o">.</span><span class="n">W</span><span class="o">.</span><span class="n">T</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">trainable</span><span class="p">:</span>
            <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">cache_in</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
                <span class="k">raise</span> <span class="ne">RuntimeError</span><span class="p">(</span><span class="s1">&#39;Gradient cache not defined. When training the train argument must be set to true in the forward pass.&#39;</span><span class="p">)</span>
            <span class="n">X</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">cache_in</span>
            <span class="n">db</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">dY</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">keepdims</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
            <span class="n">dW</span> <span class="o">=</span> <span class="n">X</span><span class="o">.</span><span class="n">T</span><span class="nd">@dY</span>
            <span class="k">assert</span> <span class="n">X</span><span class="o">.</span><span class="n">shape</span> <span class="o">==</span> <span class="n">dX</span><span class="o">.</span><span class="n">shape</span><span class="p">,</span><span class="n">f</span><span class="s2">&quot;Dimensions of grad and variable should match, X has shape </span><span class="si">{X.shape}</span><span class="s2"> and dX has shape </span><span class="si">{dX.shape}</span><span class="s2">&quot;</span>
            <span class="k">assert</span> <span class="bp">self</span><span class="o">.</span><span class="n">W</span><span class="o">.</span><span class="n">shape</span> <span class="o">==</span> <span class="n">dW</span><span class="o">.</span><span class="n">shape</span><span class="p">,</span><span class="n">f</span><span class="s2">&quot;Dimensions of grad and variable should match, W has shape </span><span class="si">{self.W.shape}</span><span class="s2"> and dW has shape </span><span class="si">{dW.shape}</span><span class="s2">&quot;</span>
            <span class="k">assert</span> <span class="bp">self</span><span class="o">.</span><span class="n">b</span><span class="o">.</span><span class="n">shape</span> <span class="o">==</span> <span class="n">db</span><span class="o">.</span><span class="n">shape</span><span class="p">,</span><span class="n">f</span><span class="s2">&quot;Dimensions of grad and variable should match, b has shape </span><span class="si">{self.b.shape}</span><span class="s2"> and db has shape </span><span class="si">{db.shape}</span><span class="s2">&quot;</span>
            <span class="k">return</span> <span class="n">dX</span><span class="p">,</span> <span class="p">[(</span><span class="bp">self</span><span class="o">.</span><span class="n">W</span><span class="p">,</span> <span class="n">dW</span><span class="p">),</span> <span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">b</span><span class="p">,</span> <span class="n">db</span><span class="p">)]</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="k">return</span> <span class="n">dX</span><span class="p">,</span> <span class="p">[]</span></div>

    <span class="k">def</span> <span class="nf">__repr__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="k">return</span> <span class="n">f</span><span class="s1">&#39;Dense Layer with shape </span><span class="si">{self.W.shape}</span><span class="s1">&#39;</span></div>

<span class="c1">#adding aliases</span>
<span class="n">Linear</span> <span class="o">=</span> <span class="n">Dense</span>

<div class="viewcode-block" id="ReLU"><a class="viewcode-back" href="../layers.html#layers.ReLU">[docs]</a><span class="k">class</span> <span class="nc">ReLU</span><span class="p">(</span><span class="n">Layer</span><span class="p">):</span>
    <span class="sd">&#39;&#39;&#39;</span>
<span class="sd">    RelU layer</span>
<span class="sd">    Represent a nonlinear transformation Y = max(0,X)</span>
<span class="sd">    &#39;&#39;&#39;</span>

    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span><span class="o">*</span><span class="p">,</span><span class="n">trainable</span><span class="o">=</span><span class="kc">True</span><span class="p">):</span>
        <span class="sd">&#39;&#39;&#39;</span>
<span class="sd">        Initialization :</span>
<span class="sd">            Does nothing since nothing to initialize</span>
<span class="sd">        &#39;&#39;&#39;</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">cache_in</span> <span class="o">=</span> <span class="kc">None</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">trainable</span> <span class="o">=</span> <span class="n">trainable</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">l_name</span> <span class="o">=</span> <span class="s1">&#39;ReLU&#39;</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">params</span> <span class="o">=</span> <span class="p">[]</span>

<div class="viewcode-block" id="ReLU.forward"><a class="viewcode-back" href="../layers.html#layers.ReLU.forward">[docs]</a>    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">X</span><span class="p">,</span> <span class="n">train</span><span class="o">=</span><span class="kc">True</span><span class="p">):</span>
        <span class="sd">&#39;&#39;&#39;</span>
<span class="sd">        Performs a forward pass through the ReLU Layer</span>

<span class="sd">        Args:</span>
<span class="sd">            :X (numpy.ndarray): Input array</span>
<span class="sd">            :train (bool): Set to True to enable gardient caching for backward step</span>

<span class="sd">        Returns:</span>
<span class="sd">            :Out (numpy.ndarray): Output after applying transformation Y = max(0,X)</span>
<span class="sd">        &#39;&#39;&#39;</span>
        <span class="n">out</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">maximum</span><span class="p">(</span><span class="n">X</span><span class="p">,</span><span class="mi">0</span><span class="p">)</span>
        <span class="k">if</span> <span class="n">train</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">cache_in</span> <span class="o">=</span> <span class="n">X</span>
        <span class="k">return</span> <span class="n">out</span></div>

<div class="viewcode-block" id="ReLU.backward"><a class="viewcode-back" href="../layers.html#layers.ReLU.backward">[docs]</a>    <span class="k">def</span> <span class="nf">backward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">dY</span><span class="p">):</span>
        <span class="sd">&#39;&#39;&#39;</span>
<span class="sd">        Performs a backward pass through the ReLU Layer</span>

<span class="sd">        Args:</span>
<span class="sd">            :dY (numpy.ndarray): Output gradient backpropagated from layers in the front</span>

<span class="sd">        Returns:</span>
<span class="sd">            :dX (numpy.ndarray): Input gradient after backpropagating dY through ReLU layer</span>
<span class="sd">            :var_grad_list (list): [], since it has no parameter to be learned</span>
<span class="sd">        &#39;&#39;&#39;</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">cache_in</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
            <span class="k">raise</span> <span class="ne">RuntimeError</span><span class="p">(</span><span class="s1">&#39;Gradient cache not defined. When training the train argument must be set to true in the forward pass.&#39;</span><span class="p">)</span>
        <span class="n">dX</span> <span class="o">=</span> <span class="n">dY</span><span class="o">*</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">cache_in</span><span class="o">&gt;=</span><span class="mi">0</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">dX</span> <span class="p">,[]</span></div></div>

<span class="c1"># adding aliases</span>
<span class="n">relu</span> <span class="o">=</span> <span class="n">ReLU</span>

<div class="viewcode-block" id="Sigmoid"><a class="viewcode-back" href="../layers.html#layers.Sigmoid">[docs]</a><span class="k">class</span> <span class="nc">Sigmoid</span><span class="p">(</span><span class="n">Layer</span><span class="p">):</span>
    <span class="sd">&#39;&#39;&#39;</span>
<span class="sd">    Sigmoid layer</span>
<span class="sd">    Represent a nonlinear transformation Y = 1/(1+e^(-X))</span>
<span class="sd">    &#39;&#39;&#39;</span>

    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span><span class="o">*</span><span class="p">,</span><span class="n">trainable</span><span class="o">=</span><span class="kc">True</span><span class="p">):</span>
        <span class="sd">&#39;&#39;&#39;</span>
<span class="sd">        Initialization:</span>
<span class="sd">            Does nothing since nothing to initialize</span>
<span class="sd">        &#39;&#39;&#39;</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">cache_in</span> <span class="o">=</span> <span class="kc">None</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">trainable</span> <span class="o">=</span> <span class="n">trainable</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">l_name</span> <span class="o">=</span> <span class="s1">&#39;Sigmoid&#39;</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">params</span> <span class="o">=</span> <span class="p">[]</span>

<div class="viewcode-block" id="Sigmoid.forward"><a class="viewcode-back" href="../layers.html#layers.Sigmoid.forward">[docs]</a>    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">X</span><span class="p">,</span> <span class="n">train</span><span class="o">=</span><span class="kc">True</span><span class="p">):</span>
        <span class="sd">&#39;&#39;&#39;</span>
<span class="sd">        Performs a forward pass through the Sigmoid Layer</span>

<span class="sd">        Args:</span>
<span class="sd">            :X (numpy.ndarray): Input array</span>
<span class="sd">            :train (bool): Set to True to enable caching for backward step</span>

<span class="sd">        Returns:</span>
<span class="sd">            :Out (numpy.ndarray): Output after applying transformation Y = 1/(1+e^(-X))</span>
<span class="sd">        &#39;&#39;&#39;</span>
        <span class="n">out</span> <span class="o">=</span> <span class="mi">1</span><span class="o">/</span><span class="p">(</span><span class="mi">1</span><span class="o">+</span><span class="n">np</span><span class="o">.</span><span class="n">exp</span><span class="p">(</span><span class="o">-</span><span class="n">X</span><span class="p">))</span>
        <span class="k">if</span> <span class="n">train</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">cache_in</span> <span class="o">=</span> <span class="n">out</span>
        <span class="k">return</span> <span class="n">out</span></div>

<div class="viewcode-block" id="Sigmoid.backward"><a class="viewcode-back" href="../layers.html#layers.Sigmoid.backward">[docs]</a>    <span class="k">def</span> <span class="nf">backward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">dY</span><span class="p">):</span>
        <span class="sd">&#39;&#39;&#39;</span>
<span class="sd">        Performs a backward pass through the Sigmoid Layer</span>

<span class="sd">        Args:</span>
<span class="sd">            :dY (numpy.ndarray): Output gradient backpropagated from layers in the front</span>

<span class="sd">        Returns:</span>
<span class="sd">            :dX (numpy.ndarray): Input gradient after backpropagating dY through Sigmoid layer</span>
<span class="sd">            :var_grad_list (list): [], since it has no parameter to be learned</span>
<span class="sd">        &#39;&#39;&#39;</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">cache_in</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
            <span class="k">raise</span> <span class="ne">RuntimeError</span><span class="p">(</span><span class="s1">&#39;Gradient cache not defined. When training the train argument must be set to true in the forward pass.&#39;</span><span class="p">)</span>
        <span class="n">out</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">cache_in</span>
        <span class="n">dX</span> <span class="o">=</span> <span class="n">dY</span><span class="o">*</span><span class="p">(</span><span class="n">out</span><span class="o">*</span><span class="p">(</span><span class="mi">1</span><span class="o">-</span><span class="n">out</span><span class="p">))</span>
        <span class="k">return</span> <span class="n">dX</span> <span class="p">,[]</span></div></div>

<div class="viewcode-block" id="Tanh"><a class="viewcode-back" href="../layers.html#layers.Tanh">[docs]</a><span class="k">class</span> <span class="nc">Tanh</span><span class="p">(</span><span class="n">Layer</span><span class="p">):</span>
    <span class="sd">&#39;&#39;&#39;</span>
<span class="sd">    Tanh layer</span>
<span class="sd">    Represent a nonlinear transformation Y = (1-e^(-2X))/(1+e^(-2X)) {tanh}</span>
<span class="sd">    &#39;&#39;&#39;</span>
    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span><span class="o">*</span><span class="p">,</span><span class="n">trainable</span><span class="o">=</span><span class="kc">True</span><span class="p">):</span>
        <span class="sd">&#39;&#39;&#39;</span>
<span class="sd">        Initialization :</span>
<span class="sd">            Does nothing since nothing to initialize</span>
<span class="sd">        &#39;&#39;&#39;</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">cache_in</span> <span class="o">=</span> <span class="kc">None</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">trainable</span> <span class="o">=</span> <span class="n">trainable</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">l_name</span> <span class="o">=</span> <span class="s1">&#39;Tanh&#39;</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">params</span> <span class="o">=</span> <span class="p">[]</span>

<div class="viewcode-block" id="Tanh.forward"><a class="viewcode-back" href="../layers.html#layers.Tanh.forward">[docs]</a>    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">X</span><span class="p">,</span> <span class="n">train</span><span class="o">=</span><span class="kc">True</span><span class="p">):</span>
        <span class="sd">&#39;&#39;&#39;</span>
<span class="sd">        Performs a forward pass through the Tanh Layer</span>

<span class="sd">        Args:</span>
<span class="sd">            :X (numpy.ndarray): Input array</span>
<span class="sd">            :train (bool): Set to True to enable caching for backward step</span>

<span class="sd">        Returns:</span>
<span class="sd">            :Out (numpy.ndarray): Output after applying transformation Y = tanh(X)</span>
<span class="sd">        &#39;&#39;&#39;</span>
        <span class="n">out</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">tanh</span><span class="p">(</span><span class="n">X</span><span class="p">)</span>
        <span class="k">if</span> <span class="n">train</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">cache_in</span> <span class="o">=</span> <span class="n">out</span>
        <span class="k">return</span> <span class="n">out</span></div>

<div class="viewcode-block" id="Tanh.backward"><a class="viewcode-back" href="../layers.html#layers.Tanh.backward">[docs]</a>    <span class="k">def</span> <span class="nf">backward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">dY</span><span class="p">):</span>
        <span class="sd">&#39;&#39;&#39;</span>
<span class="sd">        Performs a backward pass through the Tanh Layer</span>

<span class="sd">        Args:</span>
<span class="sd">            :dY (numpy.ndarray): Output gradient backpropagated from layers in the front</span>

<span class="sd">        Returns:</span>
<span class="sd">            :dX (numpy.ndarray): Input gradient after backpropagating dY through Tanh layer</span>
<span class="sd">            :var_grad_list (list): [], since it has no parameter to be learned</span>
<span class="sd">        &#39;&#39;&#39;</span>

        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">cache_in</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
            <span class="k">raise</span> <span class="ne">RuntimeError</span><span class="p">(</span><span class="s1">&#39;Gradient cache not defined. When training the train argument must be set to true in the forward pass.&#39;</span><span class="p">)</span>
        <span class="n">out</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">cache_in</span>
        <span class="n">dX</span> <span class="o">=</span> <span class="n">dY</span><span class="o">*</span><span class="p">(</span><span class="mi">1</span><span class="o">-</span><span class="n">out</span><span class="o">**</span><span class="mi">2</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">dX</span> <span class="p">,[]</span></div></div>

<div class="viewcode-block" id="BN_mean"><a class="viewcode-back" href="../layers.html#layers.BN_mean">[docs]</a><span class="k">class</span> <span class="nc">BN_mean</span><span class="p">(</span><span class="n">Layer</span><span class="p">):</span>
    <span class="sd">&#39;&#39;&#39;</span>
<span class="sd">    Mean only Batch normalization  Layer</span>

<span class="sd">    During Train:</span>
<span class="sd">        BN(x) = X - mean(X_batch) + beta</span>

<span class="sd">    During Test:</span>
<span class="sd">        BN(x) = X - mean_learned + beta</span>

<span class="sd">    Initialization:</span>
<span class="sd">        :beta: initialized to zero</span>
<span class="sd">        :mean_learned: initialized to zero</span>

<span class="sd">    Args:</span>
<span class="sd">        :dim (int): size of input passed</span>
<span class="sd">        :elr (float): exponential learning rate for updating mean_learned</span>
<span class="sd">        :trainable (bool):</span>
<span class="sd">            :False: parameters of the layer are frozed</span>
<span class="sd">            :True: parameters are updated during optimizer step</span>
<span class="sd">    &#39;&#39;&#39;</span>
    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span><span class="n">dim</span><span class="p">,</span><span class="o">*</span><span class="p">,</span><span class="n">elr</span><span class="o">=</span><span class="mf">0.9</span><span class="p">,</span><span class="n">trainable</span><span class="o">=</span><span class="kc">True</span><span class="p">):</span>
        <span class="sd">&#39;&#39;&#39;</span>
<span class="sd">        Initializes the BN_mean layer parameter</span>
<span class="sd">            beta is initialized to zero</span>
<span class="sd">            mean_learned is initialized to zero</span>

<span class="sd">        Args:</span>
<span class="sd">            dim (int)         : size of input passed</span>
<span class="sd">            elr (float)       : exponential learning rate for updating mean_learned</span>
<span class="sd">            trainable (bool)  : if set to False parameters of the layer are frozed</span>
<span class="sd">                                if set to True parameters are updated during optimizer step</span>
<span class="sd">        &#39;&#39;&#39;</span>
        <span class="k">assert</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">elr</span><span class="p">,</span><span class="nb">float</span><span class="p">)</span> <span class="ow">and</span> <span class="p">(</span><span class="n">elr</span><span class="o">&gt;</span><span class="mi">0</span> <span class="ow">and</span> <span class="n">elr</span><span class="o">&lt;</span><span class="mi">1</span><span class="p">),</span> <span class="n">f</span><span class="s1">&#39;should be float value between 0 and 1 but given </span><span class="si">{elr}</span><span class="s1">&#39;</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">beta</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">((</span><span class="mi">1</span><span class="p">,</span><span class="nb">int</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">prod</span><span class="p">(</span><span class="n">dim</span><span class="p">))))</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">cache_in</span> <span class="o">=</span> <span class="kc">None</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">mean_learned</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros_like</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">beta</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">elr</span> <span class="o">=</span> <span class="n">elr</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">trainable</span><span class="o">=</span><span class="n">trainable</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">l_name</span> <span class="o">=</span> <span class="s1">&#39;Mean only Batchnorm&#39;</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">params</span> <span class="o">=</span> <span class="p">[</span><span class="bp">self</span><span class="o">.</span><span class="n">beta</span><span class="p">]</span>

<div class="viewcode-block" id="BN_mean.forward"><a class="viewcode-back" href="../layers.html#layers.BN_mean.forward">[docs]</a>    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">X</span><span class="p">,</span> <span class="n">train</span><span class="o">=</span><span class="kc">True</span><span class="p">):</span>
        <span class="sd">&#39;&#39;&#39;</span>
<span class="sd">        Performs a forward pass through the BN_mean Layer</span>

<span class="sd">        Args:</span>
<span class="sd">            :X (numpy.ndarray): Input array</span>
<span class="sd">            :train (bool): Set to True to enable caching for backward step</span>

<span class="sd">        Returns:</span>
<span class="sd">            :Out (numpy.ndarray): Output after applying BN_mean() transformation</span>
<span class="sd">        &#39;&#39;&#39;</span>
        <span class="n">X_shape</span> <span class="o">=</span> <span class="n">X</span><span class="o">.</span><span class="n">shape</span>
        <span class="n">X_flat</span> <span class="o">=</span> <span class="n">X</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="n">X_shape</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span><span class="o">-</span><span class="mi">1</span><span class="p">)</span>
        <span class="k">if</span> <span class="n">train</span><span class="p">:</span>
            <span class="n">current_mean</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">X_flat</span><span class="p">,</span><span class="n">axis</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span><span class="n">keepdims</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
            <span class="n">out_flat</span> <span class="o">=</span> <span class="n">X_flat</span> <span class="o">-</span> <span class="n">current_mean</span> <span class="o">+</span> <span class="bp">self</span><span class="o">.</span><span class="n">beta</span>

            <span class="c1">#update for mean_learned (exponential moving average no bias currection since we are going to be trainig it sufficiently)</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">mean_learned</span> <span class="o">=</span> <span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">elr</span><span class="o">*</span><span class="bp">self</span><span class="o">.</span><span class="n">mean_learned</span><span class="p">)</span> <span class="o">+</span> <span class="p">(</span><span class="n">current_mean</span><span class="o">*</span><span class="p">(</span><span class="mi">1</span><span class="o">-</span><span class="bp">self</span><span class="o">.</span><span class="n">elr</span><span class="p">))</span>

        <span class="k">else</span><span class="p">:</span> <span class="c1">#during test use mean_learned</span>
            <span class="n">out_flat</span> <span class="o">=</span> <span class="n">X_flat</span> <span class="o">-</span> <span class="bp">self</span><span class="o">.</span><span class="n">mean_learned</span> <span class="o">+</span> <span class="bp">self</span><span class="o">.</span><span class="n">beta</span>
        <span class="k">return</span> <span class="n">out_flat</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="n">X_shape</span><span class="p">)</span></div>

<div class="viewcode-block" id="BN_mean.backward"><a class="viewcode-back" href="../layers.html#layers.BN_mean.backward">[docs]</a>    <span class="k">def</span> <span class="nf">backward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">dY</span><span class="p">):</span>
        <span class="sd">&#39;&#39;&#39;</span>
<span class="sd">        Performs a backward pass through the BN_mean Layer</span>

<span class="sd">        Args:</span>
<span class="sd">            :dY (numpy.ndarray): Output gradient backpropagated from layers in the front</span>

<span class="sd">        Returns:</span>
<span class="sd">            :dX (numpy.ndarray): Input gradient after backpropagating dY through BN_mean layer</span>
<span class="sd">            :var_grad_list (list):</span>
<span class="sd">                :trainable = True: [(beta,dbeta)]</span>
<span class="sd">                :trainable = False: [ ]</span>
<span class="sd">        &#39;&#39;&#39;</span>
        <span class="n">dY_shape</span> <span class="o">=</span> <span class="n">dY</span><span class="o">.</span><span class="n">shape</span>
        <span class="n">dY_flat</span> <span class="o">=</span> <span class="n">dY</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="n">dY_shape</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span><span class="o">-</span><span class="mi">1</span><span class="p">)</span>
        <span class="n">N</span><span class="p">,</span><span class="n">D</span> <span class="o">=</span> <span class="n">dY_flat</span><span class="o">.</span><span class="n">shape</span>
        <span class="n">dx1</span> <span class="o">=</span> <span class="n">dY_flat</span>
        <span class="n">dx2</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">ones</span><span class="p">((</span><span class="n">N</span><span class="p">,</span><span class="n">D</span><span class="p">))</span><span class="o">/</span><span class="n">N</span> <span class="o">*</span> <span class="o">-</span><span class="mi">1</span> <span class="o">*</span> <span class="n">np</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">dY_flat</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
        <span class="n">dX_flat</span> <span class="o">=</span> <span class="n">dx1</span> <span class="o">+</span> <span class="n">dx2</span>
        <span class="n">dX</span> <span class="o">=</span> <span class="n">dX_flat</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="n">dY_shape</span><span class="p">)</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">trainable</span><span class="p">:</span>
            <span class="n">dbeta</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">dY_flat</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">keepdims</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
            <span class="k">return</span> <span class="n">dX</span><span class="p">,</span> <span class="p">[(</span><span class="bp">self</span><span class="o">.</span><span class="n">beta</span><span class="p">,</span> <span class="n">dbeta</span><span class="p">)]</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="k">return</span> <span class="n">dX</span><span class="p">,[]</span></div></div>

<div class="viewcode-block" id="BN"><a class="viewcode-back" href="../layers.html#layers.BN">[docs]</a><span class="k">class</span> <span class="nc">BN</span><span class="p">(</span><span class="n">Layer</span><span class="p">):</span>
    <span class="sd">&#39;&#39;&#39;</span>
<span class="sd">    Batch normalization  Layer (Full)</span>

<span class="sd">    During Train:</span>
<span class="sd">        BN(x) = gamma(X - mean(X_batch))/std(X_batch) + beta</span>

<span class="sd">    During Test:</span>
<span class="sd">        BN(x) = gamma((X - mean_learned)/Learned_std) + beta</span>

<span class="sd">    Initialization:</span>
<span class="sd">        :beta: initialized to zeros</span>
<span class="sd">        :gamma: initialized to zeros</span>
<span class="sd">        :mean_learned: initialized to zeros</span>
<span class="sd">        :var_learned: initialized to zeros</span>

<span class="sd">    Args:</span>
<span class="sd">        :dim (int): size of input passed</span>
<span class="sd">        :elr (float): exponential learning rate for updating mean_learned</span>
<span class="sd">        :trainable (bool):</span>
<span class="sd">            :False: parameters of the layer are frozed</span>
<span class="sd">            :True: parameters are updated during optimizer step</span>
<span class="sd">    &#39;&#39;&#39;</span>
    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span><span class="n">dim</span><span class="p">,</span><span class="o">*</span><span class="p">,</span><span class="n">elr</span><span class="o">=</span><span class="mf">0.9</span><span class="p">,</span><span class="n">trainable</span><span class="o">=</span><span class="kc">True</span><span class="p">):</span>
        <span class="sd">&#39;&#39;&#39;</span>
<span class="sd">        Initializes the BN layer parameter</span>
<span class="sd">            beta is initialized to zeros</span>
<span class="sd">            gamma is initialized to zeros</span>
<span class="sd">            mean_learned is initialized to zeros</span>
<span class="sd">            var_learned is initialized to zeros</span>

<span class="sd">        Args:</span>
<span class="sd">            dim (int)         : size of input passed</span>
<span class="sd">            elr (float)       : exponential learning rate for updating mean_learned</span>
<span class="sd">            trainable (bool)  : if set to False parameters of the layer are frozed</span>
<span class="sd">                                if set to True parameters are updated during optimizer step</span>

<span class="sd">        &#39;&#39;&#39;</span>
        <span class="k">assert</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">elr</span><span class="p">,</span><span class="nb">float</span><span class="p">)</span> <span class="ow">and</span> <span class="p">(</span><span class="n">elr</span><span class="o">&gt;</span><span class="mi">0</span> <span class="ow">and</span> <span class="n">elr</span><span class="o">&lt;</span><span class="mi">1</span><span class="p">),</span> <span class="n">f</span><span class="s1">&#39;should be float value between 0 and 1 but given </span><span class="si">{elr}</span><span class="s1">&#39;</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">beta</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">((</span><span class="mi">1</span><span class="p">,</span><span class="nb">int</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">prod</span><span class="p">(</span><span class="n">dim</span><span class="p">))))</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">gamma</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">((</span><span class="mi">1</span><span class="p">,</span><span class="nb">int</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">prod</span><span class="p">(</span><span class="n">dim</span><span class="p">))))</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">cache_in</span> <span class="o">=</span> <span class="kc">None</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">mean_learned</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros_like</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">beta</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">var_learned</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros_like</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">gamma</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">elr</span> <span class="o">=</span> <span class="n">elr</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">trainable</span><span class="o">=</span><span class="n">trainable</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">l_name</span> <span class="o">=</span> <span class="s1">&#39;Batchnorm&#39;</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">eps</span><span class="o">=</span><span class="mf">1e-10</span> <span class="c1">#to avoid division_by_zero error if var = 0</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">params</span> <span class="o">=</span> <span class="p">[</span><span class="bp">self</span><span class="o">.</span><span class="n">gamma</span><span class="p">,</span><span class="bp">self</span><span class="o">.</span><span class="n">beta</span><span class="p">]</span>

<div class="viewcode-block" id="BN.forward"><a class="viewcode-back" href="../layers.html#layers.BN.forward">[docs]</a>    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">X</span><span class="p">,</span> <span class="n">train</span><span class="o">=</span><span class="kc">True</span><span class="p">):</span>
        <span class="sd">&#39;&#39;&#39;</span>
<span class="sd">        Performs a forward pass through the BN Layer</span>

<span class="sd">        Args:</span>
<span class="sd">            :X (numpy.ndarray): Input array</span>
<span class="sd">            :train (bool): Set to True to enable caching for backward step</span>

<span class="sd">        Returns:</span>
<span class="sd">            :Out (numpy.ndarray): Output after applying BN() transformation</span>
<span class="sd">        &#39;&#39;&#39;</span>
        <span class="n">X_shape</span> <span class="o">=</span> <span class="n">X</span><span class="o">.</span><span class="n">shape</span>
        <span class="n">X_flat</span> <span class="o">=</span> <span class="n">X</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="n">X_shape</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span><span class="o">-</span><span class="mi">1</span><span class="p">)</span>
        <span class="k">if</span> <span class="n">train</span><span class="p">:</span>
            <span class="k">assert</span> <span class="n">X_shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">&gt;</span><span class="mi">1</span><span class="p">,</span><span class="s2">&quot;Batch_norm layer is not supported in training mode&quot;</span>
            <span class="n">current_mean</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">X_flat</span><span class="p">,</span><span class="n">axis</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
            <span class="n">current_var</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">var</span><span class="p">(</span><span class="n">X_flat</span><span class="p">,</span><span class="n">axis</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
            <span class="n">X_norm_flat</span> <span class="o">=</span> <span class="p">(</span><span class="n">X_flat</span> <span class="o">-</span> <span class="n">current_mean</span><span class="p">)</span><span class="o">/</span><span class="n">np</span><span class="o">.</span><span class="n">sqrt</span><span class="p">(</span><span class="n">current_var</span><span class="o">+</span><span class="bp">self</span><span class="o">.</span><span class="n">eps</span><span class="p">)</span>
            <span class="n">out_flat</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">gamma</span><span class="o">*</span><span class="n">X_norm_flat</span> <span class="o">+</span> <span class="bp">self</span><span class="o">.</span><span class="n">beta</span>

            <span class="c1">#update for mean_learned and std_learned (exponential moving average no bias currection since we are going to be trainig it sufficiently)</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">mean_learned</span> <span class="o">=</span> <span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">elr</span><span class="o">*</span><span class="bp">self</span><span class="o">.</span><span class="n">mean_learned</span><span class="p">)</span> <span class="o">+</span> <span class="p">(</span><span class="n">current_mean</span><span class="o">*</span><span class="p">(</span><span class="mi">1</span><span class="o">-</span><span class="bp">self</span><span class="o">.</span><span class="n">elr</span><span class="p">))</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">var_learned</span> <span class="o">=</span> <span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">elr</span><span class="o">*</span><span class="bp">self</span><span class="o">.</span><span class="n">var_learned</span><span class="p">)</span> <span class="o">+</span> <span class="p">(</span><span class="n">current_var</span><span class="o">*</span><span class="p">(</span><span class="mi">1</span><span class="o">-</span><span class="bp">self</span><span class="o">.</span><span class="n">elr</span><span class="p">))</span>

            <span class="bp">self</span><span class="o">.</span><span class="n">cache_in</span><span class="o">=</span><span class="p">(</span><span class="n">X_flat</span><span class="p">,</span><span class="n">current_mean</span><span class="p">,</span><span class="n">current_var</span><span class="p">,</span><span class="n">X_norm_flat</span><span class="p">)</span>

        <span class="k">else</span><span class="p">:</span> <span class="c1">#during test use mean_learned adn std_learned</span>
            <span class="n">X_norm_flat</span> <span class="o">=</span><span class="p">(</span><span class="n">X_flat</span> <span class="o">-</span> <span class="bp">self</span><span class="o">.</span><span class="n">mean_learned</span><span class="p">)</span><span class="o">/</span><span class="n">np</span><span class="o">.</span><span class="n">sqrt</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">var_learned</span><span class="o">+</span><span class="bp">self</span><span class="o">.</span><span class="n">eps</span><span class="p">)</span>
            <span class="n">out_flat</span> <span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">gamma</span><span class="o">*</span><span class="n">X_norm_flat</span>  <span class="o">+</span> <span class="bp">self</span><span class="o">.</span><span class="n">beta</span>
        <span class="k">return</span> <span class="n">out_flat</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="n">X_shape</span><span class="p">)</span></div>

<div class="viewcode-block" id="BN.backward"><a class="viewcode-back" href="../layers.html#layers.BN.backward">[docs]</a>    <span class="k">def</span> <span class="nf">backward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">dY</span><span class="p">):</span>
        <span class="sd">&#39;&#39;&#39;</span>
<span class="sd">        Performs a backward pass through the BN Layer</span>

<span class="sd">        Args:</span>
<span class="sd">            :dY (numpy.ndarray): Output gradient backpropagated from layers in the front</span>

<span class="sd">        Returns:</span>
<span class="sd">            :dX (numpy.ndarray): Input gradient after backpropagating dY through BN_mean layer</span>
<span class="sd">            :var_grad_list (list):</span>
<span class="sd">                :trainable = True: [(gamma,dgamma), (beta,dbeta)]</span>
<span class="sd">                :trainable = False: []</span>
<span class="sd">        &#39;&#39;&#39;</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">cache_in</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
          <span class="k">raise</span> <span class="ne">RuntimeError</span><span class="p">(</span><span class="s1">&#39;Gradient cache not defined. When training the train argument must be set to true in the forward pass.&#39;</span><span class="p">)</span>
        <span class="n">X_flat</span><span class="p">,</span><span class="n">current_mean</span><span class="p">,</span><span class="n">current_var</span><span class="p">,</span><span class="n">X_norm_flat</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">cache_in</span>
        <span class="n">dY_shape</span> <span class="o">=</span> <span class="n">dY</span><span class="o">.</span><span class="n">shape</span>
        <span class="c1">#fatten dY</span>
        <span class="n">dY_flat</span> <span class="o">=</span> <span class="n">dY</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="n">dY_shape</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span><span class="o">-</span><span class="mi">1</span><span class="p">)</span>
        <span class="n">N</span><span class="o">=</span> <span class="n">dY_shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
        <span class="n">X_mu</span> <span class="o">=</span> <span class="n">X_flat</span> <span class="o">-</span> <span class="n">current_mean</span>
        <span class="n">inv_var</span> <span class="o">=</span> <span class="mi">1</span><span class="o">/</span><span class="n">np</span><span class="o">.</span><span class="n">sqrt</span><span class="p">(</span><span class="n">current_var</span><span class="o">+</span><span class="bp">self</span><span class="o">.</span><span class="n">eps</span><span class="p">)</span>

        <span class="n">dX_norm</span> <span class="o">=</span> <span class="n">dY_flat</span> <span class="o">*</span> <span class="bp">self</span><span class="o">.</span><span class="n">gamma</span>

        <span class="n">d_var</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">dX_norm</span><span class="o">*</span><span class="n">X_mu</span><span class="p">,</span><span class="n">axis</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span><span class="o">*</span><span class="p">(</span><span class="o">-</span><span class="p">((</span><span class="n">current_var</span><span class="o">+</span><span class="bp">self</span><span class="o">.</span><span class="n">eps</span><span class="p">)</span><span class="o">**</span><span class="p">(</span><span class="o">-</span><span class="mi">3</span><span class="o">/</span><span class="mi">2</span><span class="p">))</span><span class="o">/</span><span class="mi">2</span><span class="p">)</span>

        <span class="n">d_mu</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">dX_norm</span><span class="o">*</span><span class="p">(</span><span class="o">-</span><span class="n">inv_var</span><span class="p">),</span><span class="n">axis</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span> <span class="o">+</span> <span class="p">(</span><span class="mi">1</span><span class="o">/</span><span class="n">N</span><span class="p">)</span><span class="o">*</span><span class="n">d_var</span><span class="o">*</span><span class="n">np</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="o">-</span><span class="mi">2</span><span class="o">*</span><span class="n">X_mu</span><span class="p">,</span><span class="n">axis</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>

        <span class="n">dX_flat</span> <span class="o">=</span> <span class="p">(</span><span class="n">dX_norm</span><span class="o">*</span><span class="n">inv_var</span><span class="p">)</span><span class="o">+</span><span class="p">(</span><span class="n">d_mu</span><span class="o">+</span><span class="mi">2</span><span class="o">*</span><span class="n">d_var</span><span class="o">*</span><span class="n">X_mu</span><span class="p">)</span><span class="o">/</span><span class="n">N</span>
        <span class="n">dX</span> <span class="o">=</span> <span class="n">dX_flat</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="n">dY_shape</span><span class="p">)</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">trainable</span><span class="p">:</span>
          <span class="n">dbeta</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">dY_flat</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">keepdims</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
          <span class="n">dgamma</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">dY_flat</span><span class="o">*</span><span class="n">X_norm_flat</span><span class="p">,</span><span class="n">axis</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span><span class="n">keepdims</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
          <span class="k">return</span> <span class="n">dX</span><span class="p">,</span> <span class="p">[(</span><span class="bp">self</span><span class="o">.</span><span class="n">gamma</span><span class="p">,</span><span class="n">dgamma</span><span class="p">),(</span><span class="bp">self</span><span class="o">.</span><span class="n">beta</span><span class="p">,</span> <span class="n">dbeta</span><span class="p">)]</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="k">return</span> <span class="n">dX</span><span class="p">,[]</span></div></div>

<div class="viewcode-block" id="Flatten"><a class="viewcode-back" href="../layers.html#layers.Flatten">[docs]</a><span class="k">class</span> <span class="nc">Flatten</span><span class="p">(</span><span class="n">Layer</span><span class="p">):</span>
    <span class="sd">&#39;&#39;&#39;</span>
<span class="sd">    Flatten layer</span>
<span class="sd">    takes a tensor and converts it to a matrix</span>
<span class="sd">    This layer usually acts as an interface between conv layer and dense layer</span>
<span class="sd">    &#39;&#39;&#39;</span>
    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="sd">&#39;&#39;&#39;</span>
<span class="sd">        Initialization :</span>
<span class="sd">            Does nothing since nothing to initialize</span>
<span class="sd">        &#39;&#39;&#39;</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">cache_in</span> <span class="o">=</span> <span class="kc">None</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">l_name</span> <span class="o">=</span> <span class="s1">&#39;Flatten&#39;</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">params</span> <span class="o">=</span> <span class="p">[]</span>

<div class="viewcode-block" id="Flatten.forward"><a class="viewcode-back" href="../layers.html#layers.Flatten.forward">[docs]</a>    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">X</span><span class="p">,</span> <span class="n">train</span><span class="o">=</span><span class="kc">True</span><span class="p">):</span>
        <span class="sd">&#39;&#39;&#39;</span>
<span class="sd">        Performs a forward pass through the Flatten Layer</span>

<span class="sd">        Args:</span>
<span class="sd">            :X (numpy.ndarray): Input array</span>
<span class="sd">            :train (bool): No effect of this layer</span>

<span class="sd">        Returns:</span>
<span class="sd">            :Out (numpy.ndarray): Output after flattening</span>
<span class="sd">        &#39;&#39;&#39;</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">shape</span> <span class="o">=</span> <span class="n">X</span><span class="o">.</span><span class="n">shape</span>
        <span class="n">out</span> <span class="o">=</span> <span class="n">X</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span><span class="o">-</span><span class="mi">1</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">out</span></div>

<div class="viewcode-block" id="Flatten.backward"><a class="viewcode-back" href="../layers.html#layers.Flatten.backward">[docs]</a>    <span class="k">def</span> <span class="nf">backward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">dY</span><span class="p">):</span>
        <span class="sd">&#39;&#39;&#39;</span>
<span class="sd">        Performs a backward pass through the BN_mean Layer</span>

<span class="sd">        Args:</span>
<span class="sd">            :dY (numpy.ndarray): Output gradient backpropagated from layers in the front</span>

<span class="sd">        Returns:</span>
<span class="sd">            :dX (numpy.ndarray): Input gradient after reshaping dY</span>
<span class="sd">            :var_grad_list (list): [], since layer is not trainable</span>
<span class="sd">        &#39;&#39;&#39;</span>
        <span class="n">dX</span> <span class="o">=</span> <span class="n">dY</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">dX</span><span class="p">,[]</span></div></div>

<div class="viewcode-block" id="Conv2D"><a class="viewcode-back" href="../layers.html#layers.Conv2D">[docs]</a><span class="k">class</span> <span class="nc">Conv2D</span><span class="p">(</span><span class="n">Layer</span><span class="p">):</span>
    <span class="sd">&#39;&#39;&#39;</span>
<span class="sd">    2D Convolutional Layer</span>

<span class="sd">    Initialization:</span>
<span class="sd">        :W: initialized with either Xavier or He initialization</span>
<span class="sd">        :b: initialized to zero</span>

<span class="sd">    Args:</span>
<span class="sd">        :outChannels (int):   Number of output channels</span>
<span class="sd">        :inChannels (int):   size of output requred</span>
<span class="sd">        :filter_size (int):   Size of each kernel (filter_size x filter_size)</span>
<span class="sd">        :stride (int):   Stride to be used</span>
<span class="sd">        :padding (int):   Padding to be used for convolution</span>
<span class="sd">        :init_method (str):   initialization method to be used for Weights</span>
<span class="sd">        :trainable (bool):</span>
<span class="sd">            :False: parameters of the layer are frozed</span>
<span class="sd">            :True: parameters are updated during optimizer step</span>
<span class="sd">    &#39;&#39;&#39;</span>
    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span><span class="n">outChannels</span><span class="p">,</span><span class="n">inChannels</span><span class="p">,</span><span class="n">filter_size</span><span class="p">,</span><span class="n">stride</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span><span class="n">padding</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span><span class="o">*</span><span class="p">,</span><span class="n">init_method</span><span class="o">=</span><span class="s1">&#39;Xavier&#39;</span><span class="p">,</span><span class="n">trainable</span><span class="o">=</span><span class="kc">False</span><span class="p">):</span>
        <span class="sd">&#39;&#39;&#39;</span>
<span class="sd">        Initializes the Convolutional layer</span>
<span class="sd">            W is initialized with either Xavier or He initialization</span>
<span class="sd">            b is initialized to zero</span>

<span class="sd">        Args:</span>
<span class="sd">            outChannels (int)   :   Number of output channels</span>
<span class="sd">            inChannels (int)    :   size of output requred</span>
<span class="sd">            filter_size (int)   :   Size of each kernel (filter_size x filter_size)</span>
<span class="sd">            stride (int)        :   Stride to be used</span>
<span class="sd">            padding (int)       :   Padding to be used for convolution</span>
<span class="sd">            init_method (str)   :   initialization method to be used for Weights</span>
<span class="sd">            trainable (bool)    :   if set to False parameters of the layer are frozed</span>
<span class="sd">                                    if set to True parameters are updated during optimizer step</span>
<span class="sd">        &#39;&#39;&#39;</span>
        <span class="k">assert</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">outChannels</span><span class="p">,</span><span class="nb">int</span><span class="p">)</span> <span class="ow">and</span> <span class="n">outChannels</span> <span class="o">&gt;</span> <span class="mi">0</span>
        <span class="k">assert</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">inChannels</span><span class="p">,</span><span class="nb">int</span><span class="p">)</span> <span class="ow">and</span> <span class="n">inChannels</span> <span class="o">&gt;</span> <span class="mi">0</span>
        <span class="k">assert</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">filter_size</span><span class="p">,</span><span class="nb">int</span><span class="p">)</span> <span class="ow">and</span> <span class="n">filter_size</span> <span class="o">&gt;</span> <span class="mi">0</span>
        <span class="k">assert</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">stride</span><span class="p">,</span><span class="nb">int</span><span class="p">)</span> <span class="ow">and</span> <span class="n">stride</span> <span class="o">&gt;</span> <span class="mi">0</span>
        <span class="k">assert</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">padding</span><span class="p">,</span><span class="nb">int</span><span class="p">)</span> <span class="ow">and</span> <span class="n">padding</span> <span class="o">&gt;=</span> <span class="mi">0</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">cache_in</span> <span class="o">=</span> <span class="kc">None</span>
        <span class="n">W_size</span> <span class="o">=</span> <span class="p">(</span><span class="n">outChannels</span><span class="p">,</span><span class="n">inChannels</span><span class="p">,</span><span class="n">filter_size</span><span class="p">,</span><span class="n">filter_size</span><span class="p">)</span> <span class="c1">#currently supports only square kernels</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">W</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">rand</span><span class="p">(</span><span class="o">*</span><span class="n">W_size</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">W</span> <span class="o">*=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_initializer_</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">W</span><span class="p">,</span><span class="n">init_method</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">b</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">((</span><span class="n">outChannels</span><span class="p">,</span><span class="mi">1</span><span class="p">))</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">stride</span> <span class="o">=</span> <span class="n">stride</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">padding</span> <span class="o">=</span> <span class="n">padding</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">trainable</span> <span class="o">=</span> <span class="n">trainable</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">l_name</span> <span class="o">=</span> <span class="s1">&#39;Conv2D&#39;</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">params</span> <span class="o">=</span> <span class="p">[</span><span class="bp">self</span><span class="o">.</span><span class="n">W</span><span class="p">,</span><span class="bp">self</span><span class="o">.</span><span class="n">b</span><span class="p">]</span>

    <span class="k">def</span> <span class="nf">__repr__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="k">return</span> <span class="n">f</span><span class="s1">&#39;Conv2D Layer with </span><span class="si">{self.W.shape[0]}</span><span class="s1"> number of filters of shape </span><span class="si">{self.W.Shape[1:]}</span><span class="s1">, Stide = </span><span class="si">{self.stride}</span><span class="s1">, padding = </span><span class="si">{self.padding}</span><span class="s1"> Trainable = </span><span class="si">{self.trainable}</span><span class="s1">&#39;</span>

    <span class="nd">@staticmethod</span>
    <span class="k">def</span> <span class="nf">_convolve_</span><span class="p">(</span><span class="n">Input</span><span class="p">,</span><span class="n">kernel</span><span class="p">,</span><span class="n">bias</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span><span class="n">padding</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span><span class="n">stride</span><span class="o">=</span><span class="mi">1</span><span class="p">):</span>
        <span class="sd">&#39;&#39;&#39;</span>
<span class="sd">        2D Convolution function:</span>
<span class="sd">        Convolves Inputs with the given kernel</span>

<span class="sd">        Args:</span>
<span class="sd">            Input (numpy.ndarray)     :    Input to be Convolved over</span>
<span class="sd">            kernel (numpy.ndarray)    :    Kernel to be Convoled with</span>
<span class="sd">            bias (numpy.ndarray)      :    bias optional, set to zero unless needed</span>
<span class="sd">            padding (int)             :    padding to be used</span>
<span class="sd">            stride (int)              :    stride to used</span>

<span class="sd">        Returns:</span>
<span class="sd">            out (numpy.ndarray)       :    Output after convolution</span>
<span class="sd">            Input_col (numpy.ndarray) :    retiled and stacked input</span>
<span class="sd">        &#39;&#39;&#39;</span>
        <span class="k">assert</span> <span class="nb">len</span><span class="p">(</span><span class="n">Input</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span><span class="o">==</span><span class="mi">4</span> <span class="ow">and</span> <span class="nb">len</span><span class="p">(</span><span class="n">kernel</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span><span class="o">==</span><span class="mi">4</span>
        <span class="n">n_x</span><span class="p">,</span> <span class="n">d_x</span><span class="p">,</span> <span class="n">h_x</span><span class="p">,</span> <span class="n">w_x</span> <span class="o">=</span> <span class="n">Input</span><span class="o">.</span><span class="n">shape</span>
        <span class="n">n_filter</span><span class="p">,</span> <span class="n">d_filter</span><span class="p">,</span> <span class="n">h_filter</span><span class="p">,</span> <span class="n">w_filter</span> <span class="o">=</span> <span class="n">kernel</span><span class="o">.</span><span class="n">shape</span>
        <span class="k">assert</span> <span class="n">d_x</span> <span class="o">==</span> <span class="n">d_filter</span><span class="p">,</span><span class="s2">&quot;inputs not alligned for standard convolution&quot;</span>

        <span class="c1">#check for validity of convolution</span>
        <span class="n">h_out</span> <span class="o">=</span> <span class="p">(</span><span class="n">h_x</span> <span class="o">-</span> <span class="n">h_filter</span> <span class="o">+</span> <span class="mi">2</span> <span class="o">*</span> <span class="n">padding</span><span class="p">)</span> <span class="o">/</span> <span class="n">stride</span> <span class="o">+</span> <span class="mi">1</span>
        <span class="n">w_out</span> <span class="o">=</span> <span class="p">(</span><span class="n">w_x</span> <span class="o">-</span> <span class="n">w_filter</span> <span class="o">+</span> <span class="mi">2</span> <span class="o">*</span> <span class="n">padding</span><span class="p">)</span> <span class="o">/</span> <span class="n">stride</span> <span class="o">+</span> <span class="mi">1</span>

        <span class="k">if</span> <span class="ow">not</span> <span class="n">h_out</span><span class="o">.</span><span class="n">is_integer</span><span class="p">()</span> <span class="ow">or</span> <span class="ow">not</span> <span class="n">w_out</span><span class="o">.</span><span class="n">is_integer</span><span class="p">():</span>
            <span class="k">raise</span> <span class="ne">Exception</span><span class="p">(</span><span class="s1">&#39;Invalid output dimension!&#39;</span><span class="p">)</span>

        <span class="n">h_out</span><span class="p">,</span> <span class="n">w_out</span> <span class="o">=</span> <span class="nb">int</span><span class="p">(</span><span class="n">h_out</span><span class="p">),</span> <span class="nb">int</span><span class="p">(</span><span class="n">w_out</span><span class="p">)</span>

        <span class="n">Input_col</span> <span class="o">=</span> <span class="n">im2col_indices</span><span class="p">(</span><span class="n">Input</span><span class="p">,</span> <span class="n">h_filter</span><span class="p">,</span> <span class="n">w_filter</span><span class="p">,</span> <span class="n">padding</span><span class="o">=</span><span class="n">padding</span><span class="p">,</span> <span class="n">stride</span><span class="o">=</span><span class="n">stride</span><span class="p">)</span>
        <span class="n">kernel_row</span> <span class="o">=</span> <span class="n">kernel</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="n">n_filter</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">)</span>
        <span class="n">out</span> <span class="o">=</span> <span class="n">kernel_row</span> <span class="o">@</span> <span class="n">Input_col</span> <span class="o">+</span> <span class="n">bias</span>
        <span class="n">out</span> <span class="o">=</span> <span class="n">out</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="n">n_filter</span><span class="p">,</span> <span class="n">h_out</span><span class="p">,</span> <span class="n">w_out</span><span class="p">,</span> <span class="n">n_x</span><span class="p">)</span>
        <span class="n">out</span> <span class="o">=</span> <span class="n">out</span><span class="o">.</span><span class="n">transpose</span><span class="p">(</span><span class="mi">3</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">out</span><span class="p">,</span><span class="n">Input_col</span>


<div class="viewcode-block" id="Conv2D.forward"><a class="viewcode-back" href="../layers.html#layers.Conv2D.forward">[docs]</a>    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">X</span><span class="p">,</span> <span class="n">train</span><span class="o">=</span><span class="kc">True</span><span class="p">):</span>
        <span class="sd">&#39;&#39;&#39;</span>
<span class="sd">        Performs a forward pass through the 2D Convolution layer:</span>
<span class="sd">        Convolves Inputs with the Weights</span>

<span class="sd">        Args:</span>
<span class="sd">            :X (numpy.ndarray): Input array</span>
<span class="sd">            :train (bool): Set true to cache X and X_col</span>

<span class="sd">        Returns:</span>
<span class="sd">            :Out (numpy.ndarray): Output after Convolution</span>
<span class="sd">        &#39;&#39;&#39;</span>
        <span class="n">output</span><span class="p">,</span><span class="n">X_col</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_convolve_</span><span class="p">(</span><span class="n">Input</span> <span class="o">=</span> <span class="n">X</span><span class="p">,</span><span class="n">kernel</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">W</span><span class="p">,</span><span class="n">bias</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">b</span><span class="p">,</span><span class="n">padding</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">padding</span><span class="p">,</span> <span class="n">stride</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">stride</span><span class="p">)</span>
        <span class="k">if</span> <span class="n">train</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">cache_in</span> <span class="o">=</span> <span class="p">(</span><span class="n">X</span><span class="p">,</span><span class="n">X_col</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">output</span></div>

<div class="viewcode-block" id="Conv2D.backward"><a class="viewcode-back" href="../layers.html#layers.Conv2D.backward">[docs]</a>    <span class="k">def</span> <span class="nf">backward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">dY</span><span class="p">):</span>
        <span class="sd">&#39;&#39;&#39;</span>
<span class="sd">        Performs a backward pass through the Conv2D Layer</span>

<span class="sd">        Args:</span>
<span class="sd">            :dY (numpy.ndarray): Output gradient backpropagated from layers in the front</span>

<span class="sd">        Returns:</span>
<span class="sd">            :dX (numpy.ndarray): Input gradient after backpropagating dY through Conv2D</span>
<span class="sd">            :var_grad_list (list):</span>
<span class="sd">                :trainable = True: [(W,dW), (b,db)]</span>
<span class="sd">                :trainable = False: [ ]</span>
<span class="sd">        &#39;&#39;&#39;</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">cache_in</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
            <span class="k">raise</span> <span class="ne">RuntimeError</span><span class="p">(</span><span class="s1">&#39;Gradient cache not defined. When training the train argument must be set to true in the forward pass.&#39;</span><span class="p">)</span>
        <span class="n">X</span><span class="p">,</span><span class="n">X_col</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">cache_in</span>
        <span class="n">n_filter</span><span class="p">,</span> <span class="n">d_filter</span><span class="p">,</span> <span class="n">h_filter</span><span class="p">,</span> <span class="n">w_filter</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">W</span><span class="o">.</span><span class="n">shape</span>

        <span class="n">dY_reshaped</span> <span class="o">=</span> <span class="n">dY</span><span class="o">.</span><span class="n">transpose</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">0</span><span class="p">)</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="n">n_filter</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">)</span>

        <span class="n">W_reshape</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">W</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="n">n_filter</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">)</span>
        <span class="n">dX_col</span> <span class="o">=</span> <span class="n">W_reshape</span><span class="o">.</span><span class="n">T</span> <span class="o">@</span> <span class="n">dY_reshaped</span>
        <span class="n">dX</span> <span class="o">=</span> <span class="n">col2im_indices</span><span class="p">(</span><span class="n">dX_col</span><span class="p">,</span> <span class="n">X</span><span class="o">.</span><span class="n">shape</span><span class="p">,</span> <span class="n">h_filter</span><span class="p">,</span> <span class="n">w_filter</span><span class="p">,</span> <span class="n">padding</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">padding</span><span class="p">,</span> <span class="n">stride</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">stride</span><span class="p">)</span>
        <span class="k">assert</span> <span class="n">X</span><span class="o">.</span><span class="n">shape</span> <span class="o">==</span> <span class="n">dX</span><span class="o">.</span><span class="n">shape</span><span class="p">,</span><span class="s1">&#39;shape missmatch&#39;</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">trainable</span><span class="p">:</span>

            <span class="n">db</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">dY</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">))</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="n">n_filter</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">)</span>

            <span class="n">dW</span> <span class="o">=</span> <span class="n">dY_reshaped</span> <span class="o">@</span> <span class="n">X_col</span><span class="o">.</span><span class="n">T</span>
            <span class="n">dW</span> <span class="o">=</span> <span class="n">dW</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">W</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span>


            <span class="k">assert</span> <span class="bp">self</span><span class="o">.</span><span class="n">W</span><span class="o">.</span><span class="n">shape</span> <span class="o">==</span> <span class="n">dW</span><span class="o">.</span><span class="n">shape</span><span class="p">,</span><span class="s1">&#39;shape missmatch&#39;</span>
            <span class="k">assert</span> <span class="bp">self</span><span class="o">.</span><span class="n">b</span><span class="o">.</span><span class="n">shape</span> <span class="o">==</span> <span class="n">db</span><span class="o">.</span><span class="n">shape</span><span class="p">,</span><span class="s1">&#39;shape missmatch&#39;</span>

            <span class="k">return</span> <span class="n">dX</span><span class="p">,</span> <span class="p">[(</span><span class="bp">self</span><span class="o">.</span><span class="n">W</span><span class="p">,</span><span class="n">dW</span><span class="p">),(</span><span class="bp">self</span><span class="o">.</span><span class="n">b</span><span class="p">,</span><span class="n">db</span><span class="p">)]</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="k">return</span> <span class="n">dX</span><span class="p">,[]</span></div></div>

<div class="viewcode-block" id="dilated_Conv2D"><a class="viewcode-back" href="../layers.html#layers.dilated_Conv2D">[docs]</a><span class="k">class</span> <span class="nc">dilated_Conv2D</span><span class="p">(</span><span class="n">Conv2D</span><span class="p">):</span>
    <span class="sd">&#39;&#39;&#39;</span>
<span class="sd">    2D Dilated Convolutional Layer</span>

<span class="sd">    Initialization:</span>
<span class="sd">        :W: initialized with either Xavier or He initialization</span>
<span class="sd">        :b: initialized to zero</span>
<span class="sd">        :dm: dilation matrix that is used to dilated the kernels</span>

<span class="sd">    Args:</span>
<span class="sd">        :outChannels (int):   Number of output channels</span>
<span class="sd">        :inChannels (int):   size of output requred</span>
<span class="sd">        :filter_size (int):   Size of each kernel (filter_size x filter_size)</span>
<span class="sd">        :dilation (int):   Dilation factor to be used</span>
<span class="sd">        :stride (int):   Stride to be used</span>
<span class="sd">        :padding (int):   Padding to be used for convolution</span>
<span class="sd">        :init_method (str):   initialization method to be used for Weights</span>
<span class="sd">        :trainable (bool):</span>
<span class="sd">            :False: parameters of the layer are frozed</span>
<span class="sd">            :True: parameters are updated during optimizer step</span>
<span class="sd">    &#39;&#39;&#39;</span>
    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span><span class="n">outChannels</span><span class="p">,</span><span class="n">inChannels</span><span class="p">,</span><span class="n">filter_size</span><span class="p">,</span><span class="n">dilation</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span><span class="n">stride</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span><span class="n">padding</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span><span class="o">*</span><span class="p">,</span><span class="n">init_method</span><span class="o">=</span><span class="s1">&#39;Xavier&#39;</span><span class="p">,</span><span class="n">trainable</span><span class="o">=</span><span class="kc">False</span><span class="p">):</span>
        <span class="sd">&#39;&#39;&#39;</span>
<span class="sd">        Initializes the Convolutional layer</span>
<span class="sd">            W is initialized with either Xavier or He initialization</span>
<span class="sd">            b is initialized to zero</span>
<span class="sd">            dm generates the dilation matrix that is used to dilated the kernels</span>

<span class="sd">        Args:</span>
<span class="sd">            outChannels (int)   :   Number of output channels</span>
<span class="sd">            inChannels (int)    :   size of output requred</span>
<span class="sd">            filter_size (int)   :   Size of each kernel (filter_size x filter_size)</span>
<span class="sd">            dilation (int)      :   Dilation factor to be used</span>
<span class="sd">            stride (int)        :   Stride to be used</span>
<span class="sd">            padding (int)       :   Padding to be used for convolution</span>
<span class="sd">            init_method (str)   :   initialization method to be used for Weights</span>
<span class="sd">            trainable (bool)    :   if set to False parameters of the layer are frozed</span>
<span class="sd">                                    if set to True parameters are updated during optimizer step</span>
<span class="sd">        &#39;&#39;&#39;</span>
        <span class="nb">super</span><span class="p">(</span><span class="n">dilated_Conv2D</span><span class="p">,</span><span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="fm">__init__</span><span class="p">(</span><span class="n">outChannels</span><span class="o">=</span><span class="n">outChannels</span><span class="p">,</span><span class="n">inChannels</span><span class="o">=</span><span class="n">inChannels</span><span class="p">,</span><span class="n">filter_size</span><span class="o">=</span><span class="n">filter_size</span><span class="p">,</span><span class="n">stride</span><span class="o">=</span><span class="n">stride</span><span class="p">,</span><span class="n">padding</span><span class="o">=</span><span class="n">padding</span><span class="p">,</span><span class="n">trainable</span><span class="o">=</span><span class="n">trainable</span><span class="p">,</span><span class="n">init_method</span><span class="o">=</span><span class="n">init_method</span><span class="p">)</span>
        <span class="k">assert</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">dilation</span><span class="p">,</span><span class="nb">int</span><span class="p">)</span> <span class="ow">and</span> <span class="n">dilation</span><span class="o">&gt;</span><span class="mi">0</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">dilation</span> <span class="o">=</span> <span class="n">dilation</span> <span class="c1">#currently supports only symmetical dilations</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">dm</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_create_dilation_mat_</span><span class="p">()</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">l_name</span> <span class="o">=</span> <span class="s1">&#39;dilated_Conv2D&#39;</span>
    <span class="k">def</span> <span class="nf">__repr__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="k">return</span> <span class="n">f</span><span class="s1">&#39;Conv2D Layer with </span><span class="si">{self.W.shape[0]}</span><span class="s1"> dilation = </span><span class="si">{dilation}</span><span class="s1"> number of filters of shape </span><span class="si">{self.W.Shape[1:]}</span><span class="s1">, Stide = </span><span class="si">{self.stride}</span><span class="s1">, padding = </span><span class="si">{self.padding}</span><span class="s1"> Trainable = </span><span class="si">{self.trainable}</span><span class="s1">&#39;</span>

    <span class="k">def</span> <span class="nf">_create_dilation_mat_</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="sd">&#39;&#39;&#39;</span>
<span class="sd">        private:</span>
<span class="sd">        generates a dilation matrix that is used to dilate the kernel</span>

<span class="sd">        Returns:</span>
<span class="sd">            dilation_mat (numpy.ndarray) : Matrix that is used to dilate the kernel</span>
<span class="sd">        &#39;&#39;&#39;</span>
        <span class="n">I</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">eye</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">W</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">2</span><span class="p">])</span>
        <span class="n">z</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">((</span><span class="mi">1</span><span class="p">,</span><span class="bp">self</span><span class="o">.</span><span class="n">W</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">2</span><span class="p">]))</span>
        <span class="n">res</span> <span class="o">=</span><span class="p">[]</span>
        <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">W</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">2</span><span class="p">]):</span>
            <span class="n">res</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">I</span><span class="p">[</span><span class="n">i</span><span class="p">])</span>
            <span class="k">for</span> <span class="n">k</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">dilation</span><span class="o">-</span><span class="mi">1</span><span class="p">):</span>
                <span class="n">res</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">z</span><span class="p">)</span>
        <span class="n">res</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">row_stack</span><span class="p">(</span><span class="n">res</span><span class="p">)</span>
        <span class="n">dilation_mat</span> <span class="o">=</span> <span class="n">res</span><span class="p">[:</span><span class="o">-</span><span class="bp">self</span><span class="o">.</span><span class="n">dilation</span><span class="o">+</span><span class="mi">1</span><span class="p">]</span>
        <span class="k">return</span> <span class="n">dilation_mat</span>

<div class="viewcode-block" id="dilated_Conv2D.forward"><a class="viewcode-back" href="../layers.html#layers.dilated_Conv2D.forward">[docs]</a>    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">X</span><span class="p">,</span> <span class="n">train</span><span class="o">=</span><span class="kc">True</span><span class="p">):</span>
        <span class="sd">&#39;&#39;&#39;</span>
<span class="sd">        Performs a forward pass through the dialted Convolution 2D layer:</span>
<span class="sd">        Convolves Inputs with the kernels after dilation</span>

<span class="sd">        Args:</span>
<span class="sd">            :X (numpy.ndarray):   Input array</span>
<span class="sd">            :train (bool):   Set true to cache X and X_col</span>

<span class="sd">        Returns:</span>
<span class="sd">            :Output (numpy.ndarray):   Output after Convolution</span>
<span class="sd">        &#39;&#39;&#39;</span>
        <span class="c1"># dilate the kernels using dilation matrix</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">W_exp</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">dm</span><span class="nd">@self</span><span class="o">.</span><span class="n">W</span><span class="nd">@self</span><span class="o">.</span><span class="n">dm</span><span class="o">.</span><span class="n">T</span>
        <span class="n">output</span><span class="p">,</span><span class="n">_</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_convolve_</span><span class="p">(</span><span class="n">Input</span><span class="o">=</span><span class="n">X</span><span class="p">,</span><span class="n">kernel</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">W_exp</span><span class="p">,</span><span class="n">bias</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">b</span><span class="p">,</span><span class="n">padding</span><span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">padding</span><span class="p">,</span><span class="n">stride</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">stride</span><span class="p">)</span>
        <span class="k">if</span> <span class="n">train</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">cache_in</span> <span class="o">=</span> <span class="n">X</span>
        <span class="k">return</span> <span class="n">output</span></div>

<div class="viewcode-block" id="dilated_Conv2D.backward"><a class="viewcode-back" href="../layers.html#layers.dilated_Conv2D.backward">[docs]</a>    <span class="k">def</span> <span class="nf">backward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">dY</span><span class="p">):</span>
        <span class="sd">&#39;&#39;&#39;</span>
<span class="sd">        Performs a backward pass through the dilated Conv2D Layer</span>

<span class="sd">        Args:</span>
<span class="sd">            :dY (numpy.ndarray): Output gradient backpropagated from layers in the front</span>

<span class="sd">        Returns:</span>
<span class="sd">            :dX (numpy.ndarray): Input gradient after backpropagating dY through dilated Conv2D</span>
<span class="sd">            :var_grad_list (list):</span>
<span class="sd">                :trainable = True: [(W,dW), (b,db)]</span>
<span class="sd">                :trainable = False: [ ]</span>
<span class="sd">        &#39;&#39;&#39;</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">cache_in</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
            <span class="k">raise</span> <span class="ne">RuntimeError</span><span class="p">(</span><span class="s1">&#39;Gradient cache not defined. When training the train argument must be set to true in the forward pass.&#39;</span><span class="p">)</span>
        <span class="n">X</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">cache_in</span>

        <span class="n">n_filter</span><span class="p">,</span> <span class="n">d_filter</span><span class="p">,</span> <span class="n">h_filter</span><span class="p">,</span> <span class="n">w_filter</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">W</span><span class="o">.</span><span class="n">shape</span>

        <span class="n">exchange_mat</span><span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">rot90</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">eye</span><span class="p">(</span><span class="n">h_filter</span><span class="p">))</span>
        <span class="n">reversed_w</span> <span class="o">=</span> <span class="n">exchange_mat</span><span class="nd">@self</span><span class="o">.</span><span class="n">W</span><span class="nd">@exchange_mat</span><span class="o">.</span><span class="n">T</span>
        <span class="n">dialated_reversed_w</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">dm</span><span class="o">@</span> <span class="n">reversed_w</span> <span class="nd">@self</span><span class="o">.</span><span class="n">dm</span><span class="o">.</span><span class="n">T</span>

        <span class="n">dX</span><span class="p">,</span><span class="n">_</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_convolve_</span><span class="p">(</span><span class="n">Input</span><span class="o">=</span><span class="n">dY</span><span class="p">,</span><span class="n">kernel</span><span class="o">=</span><span class="n">dialated_reversed_w</span><span class="o">.</span><span class="n">transpose</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span><span class="mi">0</span><span class="p">,</span><span class="mi">2</span><span class="p">,</span><span class="mi">3</span><span class="p">),</span><span class="n">padding</span><span class="o">=</span><span class="n">dialated_reversed_w</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">2</span><span class="p">]</span><span class="o">-</span><span class="mi">1</span><span class="p">)</span>
        <span class="k">assert</span> <span class="n">X</span><span class="o">.</span><span class="n">shape</span> <span class="o">==</span> <span class="n">dX</span><span class="o">.</span><span class="n">shape</span>

        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">trainable</span><span class="p">:</span>

            <span class="n">db</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">dY</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">))</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="n">n_filter</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">)</span>
            <span class="n">dW</span><span class="p">,</span><span class="n">_</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_convolve_</span><span class="p">(</span><span class="n">Input</span><span class="o">=</span><span class="n">X</span><span class="o">.</span><span class="n">transpose</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span><span class="mi">0</span><span class="p">,</span><span class="mi">2</span><span class="p">,</span><span class="mi">3</span><span class="p">),</span><span class="n">kernel</span><span class="o">=</span><span class="n">dY</span><span class="o">.</span><span class="n">transpose</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span><span class="mi">0</span><span class="p">,</span><span class="mi">2</span><span class="p">,</span><span class="mi">3</span><span class="p">),</span><span class="n">stride</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">dilation</span><span class="p">,</span><span class="n">padding</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
            <span class="n">dW</span> <span class="o">=</span> <span class="n">dW</span><span class="o">.</span><span class="n">transpose</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span><span class="mi">0</span><span class="p">,</span><span class="mi">2</span><span class="p">,</span><span class="mi">3</span><span class="p">)</span>


            <span class="k">assert</span> <span class="bp">self</span><span class="o">.</span><span class="n">W</span><span class="o">.</span><span class="n">shape</span> <span class="o">==</span> <span class="n">dW</span><span class="o">.</span><span class="n">shape</span>
            <span class="k">assert</span> <span class="bp">self</span><span class="o">.</span><span class="n">b</span><span class="o">.</span><span class="n">shape</span> <span class="o">==</span> <span class="n">db</span><span class="o">.</span><span class="n">shape</span>
            <span class="k">return</span> <span class="n">dX</span><span class="p">,</span> <span class="p">[(</span><span class="bp">self</span><span class="o">.</span><span class="n">W</span><span class="p">,</span><span class="n">dW</span><span class="p">),(</span><span class="bp">self</span><span class="o">.</span><span class="n">b</span><span class="p">,</span><span class="n">db</span><span class="p">)]</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="k">return</span> <span class="n">dX</span><span class="p">,[]</span></div></div>
</pre></div>

           </div>
           
          </div>
          <footer>
  

  <hr/>

  <div role="contentinfo">
    <p>
        &copy; Copyright 2018, Arun Joseph, Saurabh S. Kulkarni

    </p>
  </div>
  Built with <a href="http://sphinx-doc.org/">Sphinx</a> using a <a href="https://github.com/rtfd/sphinx_rtd_theme">theme</a> provided by <a href="https://readthedocs.org">Read the Docs</a>. 

</footer>

        </div>
      </div>

    </section>

  </div>
  


  

    
    
      <script type="text/javascript" id="documentation_options" data-url_root="../" src="../_static/documentation_options.js"></script>
        <script type="text/javascript" src="../_static/jquery.js"></script>
        <script type="text/javascript" src="../_static/underscore.js"></script>
        <script type="text/javascript" src="../_static/doctools.js"></script>
    

  

  <script type="text/javascript" src="../_static/js/theme.js"></script>

  <script type="text/javascript">
      jQuery(function () {
          SphinxRtdTheme.Navigation.enable(true);
      });
  </script> 

</body>
</html>